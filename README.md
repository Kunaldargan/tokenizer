# tokenizer
Python implementation of a tokenizer using Regular expression.

Tokenization is the act of breaking up a sequence of strings into pieces such as words, keywords, phrases, symbols and other elements called tokens. Tokens can be individual words, phrases or even whole sentences. In the process of tokenization, some characters like punctuation marks are discarded.
Given a character sequence and splitter function has the task of splitting string into pieces, called tokens which represented as list token.
